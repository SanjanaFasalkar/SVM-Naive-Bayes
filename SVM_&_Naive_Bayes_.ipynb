{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Question 1:\n",
        "\n",
        " -  What is a Support Vector Machine (SVM), and how does it work?\n",
        " -  answers:- upport Vector Machine (SVM) is a powerful supervised machine learning algorithm used primarily for classification tasks, but it can also be used for regression (SVR ‚Äì Support Vector Regression). SVM is particularly effective in high-dimensional spaces and when the number of dimensions is greater than the number of samples.\n",
        "\n",
        "-  How SVM Works (For Classification):\n",
        "1. Goal of SVM:\n",
        "To find the optimal hyperplane that best separates the data points of different classes.\n",
        "\n",
        "-  In 2D, this hyperplane is a line.\n",
        "\n",
        "-  In 3D, it's a plane.\n",
        "\n",
        "-  In higher dimensions, it's a hyperplane.\n",
        "\n",
        "2. Maximum Margin:\n",
        "-  SVM finds the hyperplane that maximizes the margin ‚Äî the distance between the hyperplane and the nearest data points from each class. These nearest points are called support vectors.\n",
        "\n",
        "-  3. Linear vs Non-linear SVM:\n",
        "Linear SVM: Works when data is linearly separable.\n",
        "\n",
        "Non-linear SVM: Uses the kernel trick to transform data into higher dimensions where a linear separator can be found.\n",
        "\n",
        "4. Kernel Trick:\n",
        "If the data isn‚Äôt linearly separable, SVM applies a kernel function (like polynomial, RBF, or sigmoid) to map the data into a higher-dimensional space, where a hyperplane can separate the classes.\n",
        "\n",
        "-  Key Concepts:\n",
        "Concept\tDescription\n",
        "Hyperplane\tDecision boundary that separates classes\n",
        "Margin\tDistance between the hyperplane and the closest support vectors\n",
        "Support Vectors\tData points closest to the hyperplane that influence its position and margin\n",
        "Kernel\tFunction that transforms the data into higher-dimensional space\n",
        "\n",
        "-  Common Kernel Functions:\n",
        "Kernel\tDescription\n",
        "Linear\tFor linearly separable data\n",
        "Polynomial\tFor curved boundaries\n",
        "RBF (Gaussian)\tPopular for non-linear classification\n",
        "Sigmoid\tSimilar to neural networks\n",
        "\n",
        "- Advantages of SVM:\n",
        "Works well with high-dimensional data\n",
        "\n",
        "-  Effective when the number of features is greater than the number of samples\n",
        "\n",
        "- Robust to overfitting (especially with proper kernel and regularization)\n",
        "\n",
        "- Disadvantages:\n",
        "Not suitable for very large datasets (training time increases)\n",
        "\n",
        "- Choice of kernel and parameters is crucial\n",
        "\n",
        "- Hard to interpret (especially with non-linear kernels)\n",
        "\n",
        "- Example Use Cases:\n",
        "Text classification (spam detection)\n",
        "\n",
        "- Image recognition\n",
        "\n",
        "- Handwriting digit classification\n",
        "\n",
        "- Bioinformatics (gene classification)\n",
        "\n",
        "\n",
        "Question 2:\n",
        "\n",
        "-  Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "-  answers:- Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression. One of its core ideas is to find the best hyperplane that separates classes. The concepts of Hard Margin and Soft Margin relate to how strictly this separation is enforced.\n",
        "\n",
        "-  1. Hard Margin SVM\n",
        "Definition: A Hard Margin SVM does not allow any misclassifications. It assumes that the data is linearly separable.\n",
        "\n",
        "Objective: Maximize the margin with no tolerance for errors.\n",
        "\n",
        "Constraints: All data points must lie outside the margin, and none should be on the wrong side of the hyperplane.\n",
        "\n",
        "-  Advantages:\n",
        "Simple and has strong mathematical guarantees when data is perfectly separable.\n",
        "\n",
        "-  Disadvantages:\n",
        "Sensitive to noise or outliers.\n",
        "\n",
        "Not applicable when data is not perfectly separable.\n",
        "\n",
        "-  2. Soft Margin SVM\n",
        "Definition: A Soft Margin SVM allows some misclassifications to find a better generalizing model when data is not linearly separable.\n",
        "\n",
        "Objective: Find a balance between maximizing the margin and minimizing classification error using a penalty parameter C.\n",
        "\n",
        "Slack Variables (Œæ): Introduced to allow some points to be inside the margin or misclassified.\n",
        "\n",
        "-  Advantages:\n",
        "Works better on real-world noisy datasets.\n",
        "\n",
        "Allows some flexibility for overlap or noise in data.\n",
        "\n",
        "-  Disadvantages:\n",
        "More complex than Hard Margin.\n",
        "\n",
        "Requires tuning the C parameter to avoid underfitting or overfitting.\n",
        "\n",
        "-  Comparison Table\n",
        "Feature\tHard Margin SVM\tSoft Margin SVM\n",
        "Data Assumption\tLinearly separable\tNot necessarily linearly separable\n",
        "Misclassifications\tNot allowed\tAllowed with penalty\n",
        "Robustness to Noise\tLow\tHigh\n",
        "Use of Slack Vars\tNo\tYes\n",
        "Hyperparameter (C)\tNot required\tRequired to control trade-off\n",
        "\n",
        "-  When to Use:\n",
        "Use Hard Margin when data is perfectly separable and noise-free.\n",
        "\n",
        "Use Soft Margin when data is realistic with possible overlap, noise, or outliers.\n",
        "\n",
        "Question 3:\n",
        "\n",
        "-   What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "-  answers:- The Kernel Trick in Support Vector Machines (SVM) is a mathematical technique that allows the algorithm to operate in a higher-dimensional space without explicitly transforming the data.\n",
        "\n",
        "-   Why Use the Kernel Trick?\n",
        "SVMs work best when the data is linearly separable. But in many real-world cases, data is not linearly separable in its original space. The Kernel Trick helps by mapping the data to a higher-dimensional space where a linear separator (hyperplane) can be found ‚Äî without computing the actual transformation.\n",
        "\n",
        "-  How Does It Work?\n",
        "Instead of computing the dot product in high-dimensional space, the Kernel Trick uses a kernel function\n",
        "\n",
        "- ùêæ\n",
        "(\n",
        "ùë•\n",
        ",\n",
        "ùë•\n",
        "‚Ä≤\n",
        ")\n",
        "K(x,x\n",
        "‚Ä≤\n",
        " ) that directly computes the dot product of the transformed features:\n",
        "\n",
        "\n",
        "- ùêæ\n",
        "(\n",
        "ùë•\n",
        ",\n",
        "ùë•\n",
        "‚Ä≤\n",
        ")\n",
        "=\n",
        "ùúô\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "‚ãÖ\n",
        "ùúô\n",
        "(\n",
        "ùë•\n",
        "‚Ä≤\n",
        ")\n",
        "K(x,x\n",
        "‚Ä≤\n",
        " )=œï(x)‚ãÖœï(x\n",
        "‚Ä≤\n",
        " )\n",
        "\n",
        "- Here,\n",
        "ùúô\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "œï(x) is the transformation to a higher-dimensional space.\n",
        "\n",
        "-  Example of a Kernel: Radial Basis Function (RBF) or Gaussian Kernel\n",
        "\n",
        "- ùêæ\n",
        "(\n",
        "ùë•\n",
        ",\n",
        "ùë•\n",
        "‚Ä≤\n",
        ")\n",
        "=\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "ùõæ\n",
        "‚à•\n",
        "ùë•\n",
        "‚àí\n",
        "ùë•\n",
        "‚Ä≤\n",
        "‚à•\n",
        "2\n",
        ")\n",
        "K(x,x\n",
        "‚Ä≤\n",
        " )=exp(‚àíŒ≥‚à•x‚àíx\n",
        "‚Ä≤\n",
        " ‚à•\n",
        "2\n",
        " )\n",
        "Œ≥ (gamma) controls the spread of the kernel.\n",
        "\n",
        "- It measures similarity ‚Äî closer points have higher values.\n",
        "\n",
        "-  Use Case of RBF Kernel:\n",
        "The RBF kernel is widely used when:\n",
        "\n",
        "The data is non-linearly separable.\n",
        "\n",
        "The decision boundary is complex or curved.\n",
        "\n",
        "-  Example: Classifying handwritten digits (like in the MNIST dataset), where the data has non-linear relationships. The RBF kernel allows SVM to separate digits like \"8\" and \"3\" even if their pixel values don't linearly separate in original space.\n",
        "\n",
        "-  Summary:\n",
        "The Kernel Trick allows SVMs to solve non-linear classification problems efficiently.\n",
        "\n",
        "- The RBF kernel is a popular choice for such problems due to its ability to model complex boundaries.\n",
        "\n",
        "Question 4:\n",
        "\n",
        " What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        " -  answers:- üîπ What is a Na√Øve Bayes Classifier?\n",
        "A Na√Øve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is mainly used for classification tasks and works particularly well with high-dimensional datasets like text classification (e.g., spam detection, sentiment analysis).\n",
        "\n",
        "It predicts the class of a data point by calculating the posterior probability of each class given the features and selecting the class with the highest probability.\n",
        "\n",
        "-  Bayes‚Äô Theorem (Mathematical Foundation)\n",
        "\n",
        "- ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "‚ãÖ\n",
        "- ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X)=\n",
        "P(X)\n",
        "P(X‚à£C)‚ãÖP(C)\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "- ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X) is the posterior probability of class\n",
        "ùê∂\n",
        "C given features\n",
        "ùëã\n",
        "X\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(X‚à£C) is the likelihood of features\n",
        "ùëã\n",
        "X given class\n",
        "ùê∂\n",
        "C\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "P(C) is the prior probability of class\n",
        "ùê∂\n",
        "C\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(X) is the evidence (overall probability of the features)\n",
        "\n",
        "-   Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "It‚Äôs called ‚Äúna√Øve‚Äù because it assumes that all features (attributes) are independent of each other, given the class label.\n",
        "In real-world data, this assumption is often not true (features are usually correlated), hence it's considered na√Øve.\n",
        "\n",
        "Despite this unrealistic assumption, Na√Øve Bayes often performs surprisingly well, especially in text-based tasks.\n",
        "\n",
        "-  Example Use Case\n",
        "Email Spam Detection:\n",
        "\n",
        "Features: presence of words like \"win\", \"free\", \"offer\"\n",
        "\n",
        "-  Labels: \"Spam\" or \"Not Spam\"\n",
        "Na√Øve Bayes will calculate the probability of a message being spam based on the words it contains and classify it accordingly.\n",
        "\n",
        "-  Summary\n",
        "Feature\tDescription\n",
        "Type\tProbabilistic Classifier\n",
        "Based On\tBayes‚Äô Theorem\n",
        "‚ÄúNa√Øve‚Äù Assumption\tFeature independence given the class\n",
        "Strengths\tFast, scalable, performs well on text data\n",
        "Limitations\tAssumes independence, can be inaccurate with correlated features\n",
        "\n",
        "Question 5:\n",
        "\n",
        "Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
        "When would you use each one?\n",
        "-  answers:- 1. Gaussian Na√Øve Bayes\n",
        "Description:\n",
        "Assumes that the features follow a normal (Gaussian) distribution.\n",
        "\n",
        "-  Suitable for continuous (real-valued) features.\n",
        "\n",
        "- Each feature is modeled as a Gaussian distribution with its own mean and variance, computed from the training data.\n",
        "\n",
        "- Formula:\n",
        "The likelihood of a feature value\n",
        "ùë•\n",
        "x given a class\n",
        "ùë¶\n",
        "y is:\n",
        "\n",
        "- ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ùúã\n",
        "ùúé\n",
        "ùë¶\n",
        " 2\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùúá\n",
        "ùë¶\n",
        ")\n",
        "2\n",
        "2\n",
        "ùúé\n",
        "ùë¶\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "‚Äã\n",
        " ‚à£y)=\n",
        "2œÄœÉ\n",
        "y\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "1\n",
        "‚Äã\n",
        " exp(‚àí\n",
        "2œÉ\n",
        "y\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "(x\n",
        "i\n",
        "‚Äã\n",
        " ‚àíŒº\n",
        "y\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "‚Äã\n",
        " )\n",
        "Use Case:\n",
        "When input features are continuous and normally distributed.\n",
        "\n",
        "Example: Iris dataset, spam filtering with numeric features like word frequency, sensor data.\n",
        "\n",
        "-  Multinomial Na√Øve Bayes\n",
        "Description:\n",
        "Assumes that the features are discrete counts (non-negative integers).\n",
        "\n",
        "- Often used for text classification, where features represent the frequency or count of words.\n",
        "\n",
        "- Formula:\n",
        "Each feature‚Äôs likelihood is calculated as:\n",
        "\n",
        "- ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "ùëÅ\n",
        "ùë¶\n",
        "ùëñ\n",
        "+\n",
        "ùõº\n",
        "ùëÅ\n",
        "ùë¶\n",
        "+\n",
        "ùõº\n",
        "ùëõ\n",
        "P(x\n",
        "i\n",
        "‚Äã\n",
        " ‚à£y)=\n",
        "N\n",
        "y\n",
        "‚Äã\n",
        " +Œ±n\n",
        "N\n",
        "yi\n",
        "‚Äã\n",
        " +Œ±\n",
        "‚Äã\n",
        "\n",
        "- Where:\n",
        "\n",
        "ùëÅ\n",
        "ùë¶\n",
        "ùëñ\n",
        "N\n",
        "yi\n",
        "‚Äã\n",
        "  is the count of word\n",
        "ùëñ\n",
        "i in class\n",
        "ùë¶\n",
        "y,\n",
        "\n",
        "ùõº\n",
        "Œ± is the smoothing parameter (Laplace smoothing),\n",
        "\n",
        "ùëõ\n",
        "n is the number of features (words).\n",
        "\n",
        "- Use Case:\n",
        "When features represent word counts or frequencies.\n",
        "\n",
        "Example: Document classification, email spam detection, sentiment analysis.\n",
        "\n",
        "-   Bernoulli Na√Øve Bayes\n",
        "Description:\n",
        "Assumes binary features (i.e., whether a feature is present or absent).\n",
        "\n",
        "Suitable when only presence/absence of a feature matters, not its frequency.\n",
        "\n",
        "-  Formula:\n",
        "Uses a binomial distribution to model each feature:\n",
        "\n",
        "- ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "ùëù\n",
        "ùëñ\n",
        "ùë•\n",
        "ùëñ\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "1\n",
        "‚àí\n",
        "ùë•\n",
        "ùëñ\n",
        "P(x\n",
        "i\n",
        "‚Äã\n",
        " ‚à£y)=p\n",
        "i\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " (1‚àíp\n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "1‚àíx\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "\n",
        "- Where\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àà\n",
        "{\n",
        "0\n",
        ",\n",
        "1\n",
        "}\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        " ‚àà{0,1}\n",
        "\n",
        "- Use Case:\n",
        "When features are binary (e.g., word present or not in a document).\n",
        "\n",
        "- Example: Text classification with binary bag-of-words model.\n",
        "\n",
        "-  Summary: When to Use Which?\n",
        "Variant\tFeature Type\tTypical Use Case\n",
        "Gaussian\tContinuous (Real)\tSensor data, Iris dataset, medical features\n",
        "Multinomial\tDiscrete counts\tText classification with word frequencies\n",
        "Bernoulli\tBinary (0 or 1)\tText classification with binary features\n",
        "\n",
        "-  Dataset Info:\n",
        "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n",
        "-  :- Example 1: Load the Iris Dataset\n",
        "\n",
        "       # Load dataset\n",
        "       iris = load_iris()\n",
        "\n",
        "       # Convert to DataFrame\n",
        "       \n",
        "      df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "      df_iris['target'] = iris.target\n",
        "\n",
        "      # Display the first few rows\n",
        "      print(df_iris.head())\n",
        "\n",
        "Example 2: Load the Breast Cancer Dataset\n",
        "\n",
        "       # Load dataset\n",
        "       cancer = load_breast_cancer()\n",
        "\n",
        "       # Convert to DataFrame\n",
        "       df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "       df_cancer['target'] = cancer.target\n",
        "\n",
        "       # Display the first few rows\n",
        "       print(df_cancer.head())\n",
        "\n",
        "\n",
        "       # Load dataset\n",
        "       wine = load_wine()\n",
        "\n",
        "       # Convert to DataFrame\n",
        "       df_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "       df_wine['target'] = wine.target\n",
        "\n",
        "       # Display the first few rows\n",
        "       print(df_wine.head())\n",
        "\n",
        "       # Load dataset from CSV\n",
        "       df = pd.read_csv('your_file.csv')  # Replace with your path or URL\n",
        "\n",
        "       # Show data\n",
        "       print(df.head())\n",
        "\n",
        "\n",
        "-  Question 6:\n",
        "   Write a Python program to:\n",
        "‚óè Load the Iris dataset\n",
        "‚óè Train an SVM Classifier with a linear kernel\n",
        "‚óè Print the model's accuracy and support vectors\n",
        "-  answers:-  Load the Iris dataset\n",
        "\n",
        "Train an SVM Classifier with a linear kernel\n",
        "\n",
        "    Print the model's accuracy and the support vectors\n",
        "\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Load the Iris dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train an SVM classifier with a linear kernel\n",
        "    svm_model = SVC(kernel='linear')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "    # Print support vectors\n",
        "    print(\"\\nSupport Vectors:\")\n",
        "    print(svm_model.support_vectors_)\n",
        "    Output (example):\n",
        "\n",
        "    Model Accuracy: 1.0\n",
        "\n",
        "    Support Vectors:\n",
        "    [[5.1 3.8 1.6 0.2]\n",
        "    [4.8 3.4 1.6 0.2]\n",
        "    ...\n",
        "]\n",
        "Note: The actual accuracy and support vectors may vary slightly depending on the random state and data split.     \n",
        "\n",
        "Question 7:\n",
        " Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset\n",
        "‚óè Train a Gaussian Na√Øve Bayes model\n",
        "‚óè Print its classification report including precision, recall, and F1-score.\n",
        "-  answers:- dataset and a Gaussian Na√Øve Bayes model from sklearn:\n",
        "\n",
        "       from sklearn.datasets import load_breast_cancer\n",
        "       from sklearn.model_selection import train_test_split\n",
        "       from sklearn.naive_bayes import GaussianNB\n",
        "       from sklearn.metrics import classification_report\n",
        "\n",
        "       # Load the Breast Cancer dataset\n",
        "       data = load_breast_cancer()\n",
        "       X = data.data\n",
        "       y = data.target\n",
        "\n",
        "      # Split into training and testing sets (default 75% train, 25% test)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "      # Train a Gaussian Na√Øve Bayes model\n",
        "      model = GaussianNB()\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      #  Predict on test set\n",
        "      y_pred = model.predict(X_test)\n",
        "\n",
        "      # Print classification report\n",
        "      print(\"Classification Report:\\n\")\n",
        "      print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "      Output Explanation:\n",
        "-     The classification_report provides:\n",
        "\n",
        "      Precision: True Positives / (True Positives + False Positives)\n",
        "\n",
        "      Recall: True Positives / (True Positives + False Negatives)\n",
        "\n",
        "      F1-Score: Harmonic mean of Precision and Recall\n",
        "\n",
        "\n",
        "Question 8:\n",
        "\n",
        " Write a Python program to:\n",
        "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "‚óè Print the best hyperparameters and accuracy.\n",
        "-  answers:- Loads the Wine dataset from sklearn.datasets\n",
        "\n",
        "Trains an SVM classifier\n",
        "\n",
        "Uses GridSearchCV to find the best C and gamma values\n",
        "\n",
        "Prints the best hyperparameters and accuracy\n",
        "\n",
        "    from sklearn.datasets import load_wine\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Load the Wine dataset\n",
        "    data = load_wine()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the SVM model\n",
        "    svm_model = SVC()\n",
        "\n",
        "    # Define the parameter grid\n",
        "     param_grid = {\n",
        "     'C': [0.1, 1, 10, 100],\n",
        "     ' gamma': [0.001, 0.01, 0.1, 1],\n",
        "      'kernel': ['rbf']\n",
        "     }\n",
        "\n",
        "  # Set up GridSearchCV\n",
        "   grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "   # Fit the model\n",
        "   grid_search.fit(X_train, y_train)\n",
        "\n",
        "   # Predict on the test set\n",
        "   y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "   # Output the best parameters and accuracy\n",
        "   print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "   print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "   Sample Output (May vary slightly each run):\n",
        "\n",
        "   Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
        "   Test Set Accuracy: 1.0\n",
        "\n",
        "Question 9:\n",
        "\n",
        "-  Write a Python program to:\n",
        "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "‚óè Print the model's ROC-AUC score for its predictions.\n",
        "-  answers:-  Load a synthetic text dataset (20 Newsgroups)\n",
        "\n",
        "Train a Na√Øve Bayes Classifier (MultinomialNB, suitable for text)\n",
        "\n",
        "Evaluate the model using ROC-AUC score\n",
        "\n",
        "Since ROC-AUC requires binary classification, we'll simplify the problem by selecting two categories (e.g., 'sci.space' and 'rec.autos').\n",
        "\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "    # Load binary categories for simplicity (binary classification)\n",
        "    categories = ['sci.space', 'rec.autos']\n",
        "    newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "    # Vectorize text using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(newsgroups.data)\n",
        "    y = newsgroups.target\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Train a Multinomial Na√Øve Bayes classifier\n",
        "    mod el = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities for ROC-AUC\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "    # Calculate and print ROC-AUC Score\n",
        "    auc_score = roc_auc_score(y_test, y_proba)\n",
        "    print(f\"ROC-AUC Score: {auc_score:.4f}\")\n",
        "    Notes:\n",
        "    We used MultinomialNB which is ideal for text data.\n",
        "\n",
        "    ROC-AUC requires probability scores (predict_proba), not class labels.\n",
        "\n",
        "    For multiclass datasets, you would need to use roc_auc_score(..., multi_class='ovr'), but here we stick to binary for clarity.\n",
        "\n",
        "\n",
        "Question 10:\n",
        "\n",
        " Imagine you‚Äôre working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "‚óè Text with diverse vocabulary\n",
        "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "‚óè Address class imbalance\n",
        "‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.      \n",
        "\n",
        "- answers:-   1. Preprocessing the Data\n",
        "-  a. Handling Missing Data\n",
        "Text Missing: Remove emails with no subject or body text.\n",
        "\n",
        "Other Metadata Missing: Impute with suitable values (e.g., \"unknown\" for sender, mean for numerical fields).\n",
        "\n",
        "Remove duplicates or corrupted entries.\n",
        "\n",
        "- b. Text Cleaning\n",
        "Lowercasing\n",
        "\n",
        "Removing punctuation, stop words, HTML tags\n",
        "\n",
        "Tokenization and lemmatization (using nltk or spaCy)\n",
        "\n",
        "-  c. Text Vectorization\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): Preferred over simple count vectorization for handling diverse vocabulary.\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(email_texts)\n",
        "-  2. Model Selection: Na√Øve Bayes vs. SVM\n",
        "Criteria\tNa√Øve Bayes\tSVM\n",
        "Speed\tVery fast\tSlower\n",
        "Handling Sparse Data\tExcellent\tGood\n",
        "Performance on text data\tGood with TF/word-based data\tHigh if tuned well\n",
        "Interpretability\tHigher\tLower\n",
        "Scalability\tHighly scalable\tLess scalable with large datasets\n",
        "\n",
        "-  Best Choice:\n",
        "Multinomial Na√Øve Bayes (MNB) is often preferred for text classification (esp. spam detection) because:\n",
        "\n",
        "It assumes word frequencies follow a multinomial distribution.\n",
        "\n",
        "It‚Äôs very efficient on high-dimensional, sparse datasets like TF-IDF.\n",
        "\n",
        "However, SVM with a linear kernel can work better when:\n",
        "\n",
        "Accuracy is more important than speed.\n",
        "\n",
        "You have the compute resources for tuning.\n",
        "\n",
        "-  3. Handling Class Imbalance\n",
        "Since legitimate emails > spam, we must handle this imbalance:\n",
        "\n",
        "Resampling:\n",
        "\n",
        "Oversample minority (e.g., using SMOTE)\n",
        "\n",
        "Undersample majority (risk of information loss)\n",
        "\n",
        "Class weights:\n",
        "\n",
        "For models like SVM: class_weight='balanced'\n",
        "\n",
        "For Na√Øve Bayes: custom priors or stratified sampling\n",
        "\n",
        "-  4. Model Evaluation Metrics\n",
        "Accuracy is not enough due to class imbalance. Use:\n",
        "\n",
        "Metric\tWhy Important\n",
        "Precision\tLow false positives (important in spam detection)\n",
        "Recall\tLow false negatives (don‚Äôt miss actual spam)\n",
        "F1-score\tBalances precision and recall\n",
        "ROC-AUC Score\tMeasures class separation capability\n",
        "Confusion Matrix\tGives insight into specific misclassifications\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Example output\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
        "\n",
        "-   5. Business Impact of the Solution\n",
        "Business Aspect\tImpact\n",
        "User Experience\tFilters spam effectively ‚Üí boosts trust in the platform\n",
        "Security\tReduces phishing, malware emails ‚Üí protects users & company\n",
        "Operational Efficiency\tLess manual filtering, faster email delivery\n",
        "Brand Trust\tUsers stay longer on a reliable, secure communication tool\n",
        "Data for Future Models\tLabeled spam data helps improve fraud detection systems\n",
        "\n",
        "-  Final Summary\n",
        "Use TF-IDF for vectorization.\n",
        "\n",
        "Na√Øve Bayes (Multinomial) is preferred for speed and effectiveness in text classification.\n",
        "\n",
        "Handle imbalance with SMOTE or class weights.\n",
        "\n",
        "Evaluate using precision, recall, F1-score, and AUC.\n",
        "\n",
        "The model improves user experience, security, and platform credibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "arXvM4HiuL-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qwBh-aRnuKbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iuN1MA22uPEm"
      }
    }
  ]
}